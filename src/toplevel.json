[
  {
    "key": "25UN3IZE",
    "version": 134,
    "itemType": "attachment",
    "title": "dissertation.pdf",
    "url": "https://sybrandt.com/documents/dissertation.pdf",
    "accessDate": "2023-06-26T18:18:02Z",
    "linkMode": "imported_url",
    "contentType": "application/pdf",
    "charset": "",
    "filename": "dissertation.pdf",
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-26T18:18:02Z",
    "dateModified": "2023-06-26T18:18:02Z"
  },
  {
    "key": "NLV29VSM",
    "version": 160,
    "itemType": "webpage",
    "title": "Reflecting on My Failure to Build a Billion-Dollar Company",
    "url": "https://sahillavingia.com/reflecting",
    "accessDate": "2023-06-27T21:51:13Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-27T21:51:13Z",
    "dateModified": "2023-06-27T21:51:13Z"
  },
  {
    "key": "N6IW3Z93",
    "version": 2,
    "itemType": "webpage",
    "title": "On word analogies and negative results in NLP",
    "abstractNote": "Negative results are hard to publish, and even harder to make well-known. Even when the disproved result is something as pervasive as Mikolov‚Äôs word analogies.",
    "date": "2019-07-07T12:00:47-04:00",
    "language": "en",
    "url": "https://hackingsemantics.xyz/2019/analogies/",
    "accessDate": "2023-06-19T02:36:52Z",
    "websiteTitle": "Hacking semantics",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-19T02:36:52Z",
    "dateModified": "2023-06-19T02:36:52Z"
  },
  {
    "key": "MUX4E7NE",
    "version": 144,
    "itemType": "preprint",
    "title": "Textbooks Are All You Need",
    "abstractNote": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",
    "date": "2023-06-20",
    "libraryCatalog": "arXiv.org",
    "url": "http://arxiv.org/abs/2306.11644",
    "accessDate": "2023-06-26T22:17:14Z",
    "extra": "arXiv:2306.11644 [cs]",
    "repository": "arXiv",
    "archiveID": "arXiv:2306.11644",
    "creators": [
      {
        "firstName": "Suriya",
        "lastName": "Gunasekar",
        "creatorType": "author"
      },
      { "firstName": "Yi", "lastName": "Zhang", "creatorType": "author" },
      { "firstName": "Jyoti", "lastName": "Aneja", "creatorType": "author" },
      {
        "firstName": "Caio C√©sar Teodoro",
        "lastName": "Mendes",
        "creatorType": "author"
      },
      {
        "firstName": "Allie",
        "lastName": "Del Giorno",
        "creatorType": "author"
      },
      { "firstName": "Sivakanth", "lastName": "Gopi", "creatorType": "author" },
      {
        "firstName": "Mojan",
        "lastName": "Javaheripi",
        "creatorType": "author"
      },
      {
        "firstName": "Piero",
        "lastName": "Kauffmann",
        "creatorType": "author"
      },
      {
        "firstName": "Gustavo",
        "lastName": "de Rosa",
        "creatorType": "author"
      },
      { "firstName": "Olli", "lastName": "Saarikivi", "creatorType": "author" },
      { "firstName": "Adil", "lastName": "Salim", "creatorType": "author" },
      { "firstName": "Shital", "lastName": "Shah", "creatorType": "author" },
      {
        "firstName": "Harkirat Singh",
        "lastName": "Behl",
        "creatorType": "author"
      },
      { "firstName": "Xin", "lastName": "Wang", "creatorType": "author" },
      {
        "firstName": "S√©bastien",
        "lastName": "Bubeck",
        "creatorType": "author"
      },
      { "firstName": "Ronen", "lastName": "Eldan", "creatorType": "author" },
      {
        "firstName": "Adam Tauman",
        "lastName": "Kalai",
        "creatorType": "author"
      },
      { "firstName": "Yin Tat", "lastName": "Lee", "creatorType": "author" },
      { "firstName": "Yuanzhi", "lastName": "Li", "creatorType": "author" }
    ],
    "tags": [
      { "tag": "Computer Science - Computation and Language", "type": 1 },
      { "tag": "Computer Science - Machine Learning", "type": 1 },
      { "tag": "Computer Science - Artificial Intelligence", "type": 1 }
    ],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-26T22:17:14Z",
    "dateModified": "2023-06-26T22:17:14Z"
  },
  {
    "key": "DSZHXVCP",
    "version": 103,
    "itemType": "forumPost",
    "title": "an incredible workflow to caption images",
    "date": "2023-02-05T22:41:02.000Z",
    "url": "www.reddit.com/r/StableDiffusion/comments/10upvdq/an_incredible_workflow_to_caption_images/",
    "accessDate": "2023-06-23T03:58:03Z",
    "forumTitle": "r/StableDiffusion",
    "postType": "Reddit Post",
    "creators": [
      { "firstName": "", "lastName": "ReadyAndSalted", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:58:03Z",
    "dateModified": "2023-06-23T03:58:03Z"
  },
  {
    "key": "5L32X5A8",
    "version": 43,
    "itemType": "webpage",
    "title": "Brian Hoang - I like startups",
    "abstractNote": "I like startups",
    "language": "en-US",
    "url": "https://brianqhoang.com/",
    "accessDate": "2023-06-23T03:31:19Z",
    "websiteTitle": "Brian Hoang",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:31:19Z",
    "dateModified": "2023-06-23T03:31:19Z"
  },
  {
    "key": "IWIEHPBD",
    "version": 107,
    "itemType": "webpage",
    "title": "Learning to generate line drawings that convey geometry and semantics",
    "url": "https://carolineec.github.io/informative_drawings/",
    "accessDate": "2023-06-23T04:00:48Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T04:00:48Z",
    "dateModified": "2023-06-23T04:00:48Z"
  },
  {
    "key": "5SVRM66K",
    "version": 2,
    "itemType": "conferencePaper",
    "title": "Global Landslide Forecasting and Dynamic Susceptibility Mapping Using a New Dataset of Geophysical and Meteorological Features",
    "date": "2021-10-8",
    "language": "en",
    "libraryCatalog": "DOI.org (Crossref)",
    "url": "https://ieeexplore.ieee.org/document/9701628/",
    "accessDate": "2023-06-14T17:15:53Z",
    "place": "Cambridge, MA, USA",
    "publisher": "IEEE",
    "ISBN": "978-1-66540-595-9",
    "pages": "1-6",
    "proceedingsTitle": "2021 IEEE MIT Undergraduate Research Technology Conference (URTC)",
    "conferenceName": "2021 IEEE MIT Undergraduate Research Technology Conference ((URTC))",
    "DOI": "10.1109/URTC54388.2021.9701628",
    "creators": [
      { "firstName": "Ishaan", "lastName": "Javali", "creatorType": "author" },
      { "firstName": "Shrey", "lastName": "Joshi", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "inPublications": true,
    "dateAdded": "2023-06-14T17:15:53Z",
    "dateModified": "2023-06-14T17:16:20Z"
  },
  {
    "key": "GY3FD4WM",
    "version": 143,
    "itemType": "conferencePaper",
    "title": "How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?",
    "abstractNote": "Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hyperspherical space, and demonstrate the importance of dispersion and compactness. CIDER establishes superior performance, outperforming the latest rival by 13.33% in FPR95. Code is available at https://github.com/deeplearning-wisc/cider.",
    "date": "2022/09/29",
    "language": "en",
    "libraryCatalog": "openreview.net",
    "url": "https://openreview.net/forum?id=aEFaE0W5pAd",
    "accessDate": "2023-06-26T22:17:04Z",
    "conferenceName": "The Eleventh International Conference on Learning Representations",
    "creators": [
      { "firstName": "Yifei", "lastName": "Ming", "creatorType": "author" },
      { "firstName": "Yiyou", "lastName": "Sun", "creatorType": "author" },
      { "firstName": "Ousmane", "lastName": "Dia", "creatorType": "author" },
      { "firstName": "Yixuan", "lastName": "Li", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-26T22:17:04Z",
    "dateModified": "2023-06-26T22:17:04Z"
  },
  {
    "key": "KD2XIY34",
    "version": 4,
    "itemType": "preprint",
    "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
    "abstractNote": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
    "date": "2023-06-05",
    "shortTitle": "Orca",
    "libraryCatalog": "arXiv.org",
    "url": "http://arxiv.org/abs/2306.02707",
    "accessDate": "2023-06-22T05:38:53Z",
    "extra": "arXiv:2306.02707 [cs]",
    "repository": "arXiv",
    "archiveID": "arXiv:2306.02707",
    "creators": [
      {
        "firstName": "Subhabrata",
        "lastName": "Mukherjee",
        "creatorType": "author"
      },
      { "firstName": "Arindam", "lastName": "Mitra", "creatorType": "author" },
      { "firstName": "Ganesh", "lastName": "Jawahar", "creatorType": "author" },
      { "firstName": "Sahaj", "lastName": "Agarwal", "creatorType": "author" },
      { "firstName": "Hamid", "lastName": "Palangi", "creatorType": "author" },
      { "firstName": "Ahmed", "lastName": "Awadallah", "creatorType": "author" }
    ],
    "tags": [
      { "tag": "Computer Science - Computation and Language", "type": 1 },
      { "tag": "Computer Science - Machine Learning", "type": 1 }
    ],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-22T05:38:53Z",
    "dateModified": "2023-06-22T05:38:53Z"
  },
  {
    "key": "6DHEJBLA",
    "version": 132,
    "itemType": "webpage",
    "title": "Victim Signaling and Dark Triad Personality Traits",
    "abstractNote": "How people with dark personalities use victimhood as a strategy",
    "date": "2022-04-24",
    "url": "https://www.robkhenderson.com/p/victim-signaling-and-dark-triad-personality",
    "accessDate": "2023-06-25T15:43:06Z",
    "creators": [
      { "firstName": "Rob", "lastName": "Henderson", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-25T15:43:06Z",
    "dateModified": "2023-06-25T15:43:06Z"
  },
  {
    "key": "8IVUDSX2",
    "version": 19,
    "itemType": "webpage",
    "title": "Quantitative Finance Reading List | QuantStart",
    "url": "http://www.quantstart.com/articles/Quantitative-Finance-Reading-List/",
    "accessDate": "2023-06-22T05:41:05Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-22T05:41:05Z",
    "dateModified": "2023-06-22T05:41:05Z"
  },
  {
    "key": "8A6ST67A",
    "version": 84,
    "itemType": "webpage",
    "title": "copilot-explorer",
    "abstractNote": "Hacky repo to see what the Copilot extension sends to the server",
    "language": "en-US",
    "url": "https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals.html",
    "accessDate": "2023-06-23T03:50:19Z",
    "websiteTitle": "copilot-explorer",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:50:19Z",
    "dateModified": "2023-06-23T03:50:19Z"
  },
  {
    "key": "A7TY6VW2",
    "version": 39,
    "itemType": "blogPost",
    "title": "Career Advice Nobody Gave Me: Never Ignore a Recruiter",
    "abstractNote": "They are frustrating, annoying, and one of the best career resources you can find",
    "date": "2022-02-02T17:54:52.722Z",
    "language": "en",
    "shortTitle": "Career Advice Nobody Gave Me",
    "url": "https://index.medium.com/career-advice-nobody-gave-me-never-ignore-a-recruiter-4474eac9556",
    "accessDate": "2023-06-23T03:30:24Z",
    "blogTitle": "Index",
    "creators": [
      { "firstName": "Alex", "lastName": "Chesser", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:30:24Z",
    "dateModified": "2023-06-23T03:30:24Z"
  },
  {
    "key": "D2LYWMT6",
    "version": 23,
    "itemType": "webpage",
    "title": "How Attention works in Deep Learning: understanding the attention mechanism in sequence models",
    "abstractNote": "New to Natural Language Processing? This is the ultimate beginner‚Äôs guide to the attention mechanism and sequence learning to get you started",
    "date": "2020-11-19",
    "language": "en",
    "shortTitle": "How Attention works in Deep Learning",
    "url": "https://theaisummer.com/attention/",
    "accessDate": "2023-06-23T03:26:11Z",
    "websiteTitle": "AI Summer",
    "creators": [
      {
        "firstName": "Nikolas",
        "lastName": "Adaloglou",
        "creatorType": "author"
      }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:26:11Z",
    "dateModified": "2023-06-23T03:26:11Z"
  },
  {
    "key": "LPLPRKEP",
    "version": 68,
    "itemType": "webpage",
    "title": "Determinate optimism. I have been very inspired by Peter‚Ä¶ | by tarun mehta | Medium",
    "url": "https://tarunsmehta.medium.com/determinate-optimism-fa9adab03464",
    "accessDate": "2023-06-23T03:41:36Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:41:36Z",
    "dateModified": "2023-06-23T03:41:36Z"
  },
  {
    "key": "AZDFZV69",
    "version": 3,
    "itemType": "webpage",
    "title": "The business of extracting knowledge from academic publications | Hacker News",
    "url": "https://news.ycombinator.com/item?id=29481061",
    "accessDate": "2023-06-19T02:41:23Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-19T02:41:23Z",
    "dateModified": "2023-06-19T02:41:23Z"
  },
  {
    "key": "GFHRZBSJ",
    "version": 64,
    "itemType": "journalArticle",
    "title": "The Straussian Moment",
    "language": "en",
    "libraryCatalog": "Zotero",
    "creators": [
      { "firstName": "Peter", "lastName": "Thiel", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:40:29Z",
    "dateModified": "2023-06-23T03:40:29Z"
  },
  {
    "key": "C7B4F4EF",
    "version": 122,
    "itemType": "forumPost",
    "title": "NOTE ON BUILDING I had some friends ask me about my process of learning, building, and progressing recently and wanted to share my answer publicly if it might be helpful to others. I don't read very often, I rarely listen to podcasts or watch videos around projects, or think much‚Ä¶",
    "date": "2023-06-23T21:00Z",
    "language": "en",
    "url": "https://twitter.com/willdepue/status/1672348838559092736",
    "accessDate": "2023-06-24T20:26:18Z",
    "forumTitle": "Twitter",
    "postType": "Tweet",
    "creators": [
      {
        "firstName": "",
        "lastName": "will depue [@willdepue]",
        "creatorType": "author"
      }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-24T20:26:18Z",
    "dateModified": "2023-06-24T20:26:18Z"
  },
  {
    "key": "CB3MQKP2",
    "version": 27,
    "itemType": "webpage",
    "title": "How to ‚Äúfarm‚Äù Kaggle in the right way",
    "abstractNote": "This article describes the advice and approaches on how to effectively use Kaggle to improve practical skills in Data Science.",
    "date": "2019-02-12T14:58:44.916Z",
    "language": "en",
    "url": "https://towardsdatascience.com/how-to-farm-kaggle-in-the-right-way-b27f781b78da",
    "accessDate": "2023-06-23T03:26:55Z",
    "websiteTitle": "Medium",
    "creators": [
      { "firstName": "Alex", "lastName": "Kruegger", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:26:55Z",
    "dateModified": "2023-06-23T03:26:55Z"
  },
  {
    "key": "CJEM4QUS",
    "version": 117,
    "itemType": "preprint",
    "title": "Lazy Prices",
    "abstractNote": "Using the complete history of regular quarterly and annual filings by U.S. corporations from 1995-2014, we show that when firms make an active change in their reporting practices, this conveys an important signal about future firm operations. Changes to the language and construction of financial reports also have strong implications for firms‚Äô future returns: a portfolio that shorts ‚Äúchangers‚Äù and buys ‚Äúnon-changers‚Äù earns up to 188 basis points in monthly alphas (over 22% per year) in the future. Changes in language referring to the executive (CEO and CFO) team, regarding litigation, or in the risk factor section of the documents are especially informative for future returns. We show that changes to the 10-Ks predict future earnings, profitability, future news announcements, and even future firm-level bankruptcies; meanwhile firms that do not make changes experience positive abnormal returns. Unlike typical underreaction patterns in asset prices, we find no announcement effect associated with these changes--with returns only accruing when the information is later revealed through news, events, or earnings--suggesting that investors are inattentive to these simple changes across the universe of public firms.",
    "date": "2019-03-07",
    "language": "en",
    "libraryCatalog": "Social Science Research Network",
    "url": "https://papers.ssrn.com/abstract=1658471",
    "accessDate": "2023-06-24T16:19:08Z",
    "place": "Rochester, NY",
    "DOI": "10.2139/ssrn.1658471",
    "genre": "SSRN Scholarly Paper",
    "archiveID": "1658471",
    "creators": [
      { "firstName": "Lauren", "lastName": "Cohen", "creatorType": "author" },
      {
        "firstName": "Christopher J.",
        "lastName": "Malloy",
        "creatorType": "author"
      },
      { "firstName": "Quoc", "lastName": "Nguyen", "creatorType": "author" }
    ],
    "tags": [
      { "tag": "annual reports", "type": 1 },
      { "tag": "inattention", "type": 1 },
      { "tag": "Information", "type": 1 },
      { "tag": "lazy investors", "type": 1 },
      { "tag": "predictable returns", "type": 1 }
    ],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-24T16:19:08Z",
    "dateModified": "2023-06-24T16:19:08Z"
  },
  {
    "key": "KBZ2SR3X",
    "version": 111,
    "itemType": "webpage",
    "title": "the tiny corp raised $5.1M",
    "abstractNote": "Here we go again. I started another company. The money is in the bank.",
    "date": "2023-05-24T00:00:00-07:00",
    "language": "en",
    "url": "https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html",
    "accessDate": "2023-06-23T04:02:15Z",
    "websiteTitle": "the singularity is nearer",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T04:02:15Z",
    "dateModified": "2023-06-23T04:02:15Z"
  },
  {
    "key": "MSF3GHSW",
    "version": 3,
    "itemType": "webpage",
    "title": "Basics of the Unix Philosophy",
    "url": "http://www.catb.org/esr/writings/taoup/html/ch01s06.html",
    "accessDate": "2023-06-19T02:40:04Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-19T02:40:04Z",
    "dateModified": "2023-06-19T02:40:04Z"
  },
  {
    "key": "CWRN3Z48",
    "version": 120,
    "itemType": "webpage",
    "title": "Semaphore",
    "url": "https://www.semaphore.fyi/",
    "accessDate": "2023-06-23T03:38:22Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:38:22Z",
    "dateModified": "2023-06-24T17:55:55Z"
  },
  {
    "key": "E34HWRE9",
    "version": 130,
    "itemType": "webpage",
    "title": "The Operating Manual for Your Nervous System",
    "abstractNote": "Embrace ‚Äòstate over story‚Äô to shift your thoughts and overcome anxiety",
    "date": "2023-02-24",
    "url": "https://every.to/p/the-operating-manual-for-your-nervous-system",
    "accessDate": "2023-06-25T07:28:43Z",
    "creators": [
      { "firstName": "Jonny", "lastName": "Miller", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-25T07:28:43Z",
    "dateModified": "2023-06-25T07:28:43Z"
  },
  {
    "key": "EA5NYY3R",
    "version": 3,
    "itemType": "webpage",
    "title": "THE ALEXANDRIA INDEX",
    "abstractNote": "From barbarism to civilization requires a century; from civilization to barbarism needs but a day.",
    "language": "en",
    "url": "https://alex.macrocosm.so/",
    "accessDate": "2023-06-19T02:37:48Z",
    "websiteTitle": "THE ALEXANDRIA INDEX",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-19T02:37:48Z",
    "dateModified": "2023-06-19T02:37:48Z"
  },
  {
    "key": "ETEF6RWJ",
    "version": 4,
    "itemType": "webpage",
    "title": "The Illustrated Transformer",
    "abstractNote": "Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n\nWatch: MIT‚Äôs Deep Learning State of the Art lecture referencing this post\n\nIn the previous post, we looked at Attention ‚Äì a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer ‚Äì a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud‚Äôs recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let‚Äôs try to break the model apart and look at how it functions.\n\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard‚Äôs NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n\n2020 Update: I‚Äôve created a ‚ÄúNarrated Transformer‚Äù video which is a gentler approach to the topic:\n\n\n\n\nA High-Level Look\nLet‚Äôs begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.",
    "url": "http://jalammar.github.io/illustrated-transformer/",
    "accessDate": "2023-06-22T05:26:18Z",
    "creators": [
      { "firstName": "Jay", "lastName": "Alammar", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-22T05:26:18Z",
    "dateModified": "2023-06-22T05:26:18Z"
  },
  {
    "key": "FX2SQDRW",
    "version": 96,
    "itemType": "attachment",
    "title": "MathematiciansLament",
    "url": "https://www.maa.org/external_archive/devlin/LockhartsLament.pdf",
    "accessDate": "2023-06-23T03:37:28Z",
    "linkMode": "imported_url",
    "contentType": "application/pdf",
    "charset": "",
    "filename": "LockhartsLament.pdf",
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:37:28Z",
    "dateModified": "2023-06-23T03:55:20Z"
  },
  {
    "key": "KZCPYM2K",
    "version": 152,
    "itemType": "journalArticle",
    "title": "A Generalist Agent",
    "abstractNote": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
    "date": "2022/08/29",
    "language": "en",
    "libraryCatalog": "openreview.net",
    "url": "https://openreview.net/forum?id=1ikK0kHjvj",
    "accessDate": "2023-06-26T22:44:19Z",
    "publicationTitle": "Transactions on Machine Learning Research",
    "ISSN": "2835-8856",
    "creators": [
      { "firstName": "Scott", "lastName": "Reed", "creatorType": "author" },
      { "firstName": "Konrad", "lastName": "Zolna", "creatorType": "author" },
      {
        "firstName": "Emilio",
        "lastName": "Parisotto",
        "creatorType": "author"
      },
      {
        "firstName": "Sergio G√≥mez",
        "lastName": "Colmenarejo",
        "creatorType": "author"
      },
      {
        "firstName": "Alexander",
        "lastName": "Novikov",
        "creatorType": "author"
      },
      {
        "firstName": "Gabriel",
        "lastName": "Barth-maron",
        "creatorType": "author"
      },
      { "firstName": "Mai", "lastName": "Gim√©nez", "creatorType": "author" },
      { "firstName": "Yury", "lastName": "Sulsky", "creatorType": "author" },
      { "firstName": "Jackie", "lastName": "Kay", "creatorType": "author" },
      {
        "firstName": "Jost Tobias",
        "lastName": "Springenberg",
        "creatorType": "author"
      },
      { "firstName": "Tom", "lastName": "Eccles", "creatorType": "author" },
      { "firstName": "Jake", "lastName": "Bruce", "creatorType": "author" },
      { "firstName": "Ali", "lastName": "Razavi", "creatorType": "author" },
      { "firstName": "Ashley", "lastName": "Edwards", "creatorType": "author" },
      { "firstName": "Nicolas", "lastName": "Heess", "creatorType": "author" },
      { "firstName": "Yutian", "lastName": "Chen", "creatorType": "author" },
      { "firstName": "Raia", "lastName": "Hadsell", "creatorType": "author" },
      { "firstName": "Oriol", "lastName": "Vinyals", "creatorType": "author" },
      { "firstName": "Mahyar", "lastName": "Bordbar", "creatorType": "author" },
      {
        "firstName": "Nando de",
        "lastName": "Freitas",
        "creatorType": "author"
      }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-26T22:44:19Z",
    "dateModified": "2023-06-26T22:44:19Z"
  },
  {
    "key": "UM4ZBJUQ",
    "version": 53,
    "itemType": "blogPost",
    "title": "Daisi Shutting Down",
    "abstractNote": "I‚Äôm sad to announce that Daisi Technology, Inc. will be shutting down.",
    "date": "2022-10-31T12:13:39.926Z",
    "language": "en",
    "url": "https://medium.com/daisi/daisi-shutting-down-4385bca2acfc",
    "accessDate": "2023-06-23T03:35:30Z",
    "blogTitle": "Daisi Technology",
    "creators": [
      { "firstName": "Eric", "lastName": "Tilenius", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:35:30Z",
    "dateModified": "2023-06-23T03:35:30Z"
  },
  {
    "key": "VQYMBWAH",
    "version": 98,
    "itemType": "webpage",
    "title": "Moonlight Founder Programs",
    "abstractNote": "A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team",
    "url": "https://www.notion.so",
    "accessDate": "2023-06-23T03:54:39Z",
    "websiteTitle": "Notion",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:54:39Z",
    "dateModified": "2023-06-23T03:55:56Z"
  },
  {
    "key": "IZMSU5NM",
    "version": 88,
    "itemType": "videoRecording",
    "title": "General Magic (1080p) FULL DOCUMENTARY - History, Technology, Business",
    "abstractNote": "From the first smartphones to social media, eBay to emojis, the ideas that dominate our lives were born at a 1989 tech startup you‚Äôve never heard of. ‚ÄòGeneral Magic‚Äô is a tale of how a great vision and an epic failure changed the world.\n#fullmovie #freemovie #gravitasfilms \nDirected by: Sarah Kerruish, Matt Maude\nStarring:\nTony Fadell\nAndy Hertzfeld\nMarc Porat\nüçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø \nWANT TO SEE MORE MOVIES LIKE THIS FOR FREE here on YouTube?\nPlease visit our other channels!\nGravitas Ventures:    / gravitasvod  \nDocumentaries:    / @gravitasdocument...  \nAction/Thriller/Horror:    / @gravitasadrenaline  \nSci-Fi/Paranormal:    / @gravitasunexplained  \n#GravitasFilms #HDFilms / Full length FREE MOVIES\nFollow us on Twitter: https://twitter.com/GravitasVOD\nFollow us on Instagram: https://www.instagram.com/gravitasven...\nCheck us out on FaceBook: https://www.facebook.com/GravitasVent...\nLearn more about Gravitas Ventures by visiting our website: https://gravitasventures.com/\nüçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø üé• üéûÔ∏è üçø",
    "date": "2022-10-06",
    "libraryCatalog": "YouTube",
    "url": "https://www.youtube.com/watch?v=JQymn5flcek",
    "accessDate": "2023-06-23T03:54:23Z",
    "runningTime": "1:32:47",
    "creators": [
      { "name": "Gravitas FREE DOCUMENTARIES", "creatorType": "director" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:54:24Z",
    "dateModified": "2023-06-23T03:54:24Z"
  },
  {
    "key": "QVT7NQP9",
    "version": 72,
    "itemType": "blogPost",
    "title": "Zero Knowledge Proofs: An illustrated primer",
    "abstractNote": "One of the best things about modern cryptography is the beautiful terminology. You could start any number of punk bands (or Tumblrs) named after cryptography terms like ‚Äòhard-core predicate&#‚Ä¶",
    "date": "2014-11-27T21:44:00+00:00",
    "language": "en",
    "shortTitle": "Zero Knowledge Proofs",
    "url": "https://blog.cryptographyengineering.com/2014/11/27/zero-knowledge-proofs-illustrated-primer/",
    "accessDate": "2023-06-23T03:45:18Z",
    "blogTitle": "A Few Thoughts on Cryptographic Engineering",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:45:18Z",
    "dateModified": "2023-06-23T03:45:18Z"
  },
  {
    "key": "SUZPMLAU",
    "version": 156,
    "itemType": "blogPost",
    "title": "Natural Language Is an Unnatural Interface",
    "abstractNote": "How can we use familiar design patterns to constrain the unwieldy state space of large language models?",
    "date": "2023-06-27",
    "url": "https://varunshenoy.substack.com/p/natural-language-is-an-unnatural",
    "accessDate": "2023-06-27T21:41:10Z",
    "blogTitle": "Public Experiments",
    "websiteType": "Substack newsletter",
    "creators": [
      { "firstName": "", "lastName": "Varun", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-27T21:41:10Z",
    "dateModified": "2023-06-27T21:41:10Z"
  },
  {
    "key": "K7CF6CNF",
    "version": 80,
    "itemType": "webpage",
    "title": "Understanding Large Language Models -- A Transformative Reading List",
    "abstractNote": "Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and prac...",
    "date": "08:00:00 +0000",
    "language": "en",
    "url": "https://sebastianraschka.com/blog/2023/llm-reading-list.html",
    "accessDate": "2023-06-23T03:48:03Z",
    "websiteTitle": "Sebastian Raschka, PhD",
    "creators": [
      {
        "firstName": "Sebastian",
        "lastName": "Raschka",
        "creatorType": "author"
      }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:48:03Z",
    "dateModified": "2023-06-23T03:48:03Z"
  },
  {
    "key": "LFLLPW9F",
    "version": 47,
    "itemType": "webpage",
    "title": "Michael Fogleman: Projects",
    "url": "https://www.michaelfogleman.com/",
    "accessDate": "2023-06-23T03:33:19Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:33:19Z",
    "dateModified": "2023-06-23T03:33:19Z"
  },
  {
    "key": "M574AS65",
    "version": 136,
    "itemType": "preprint",
    "title": "Scaling Expert Language Models with Unsupervised Domain Discovery",
    "abstractNote": "Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.",
    "date": "2023-03-24",
    "libraryCatalog": "arXiv.org",
    "url": "http://arxiv.org/abs/2303.14177",
    "accessDate": "2023-06-26T20:08:57Z",
    "extra": "arXiv:2303.14177 [cs]",
    "repository": "arXiv",
    "archiveID": "arXiv:2303.14177",
    "creators": [
      {
        "firstName": "Suchin",
        "lastName": "Gururangan",
        "creatorType": "author"
      },
      { "firstName": "Margaret", "lastName": "Li", "creatorType": "author" },
      { "firstName": "Mike", "lastName": "Lewis", "creatorType": "author" },
      { "firstName": "Weijia", "lastName": "Shi", "creatorType": "author" },
      { "firstName": "Tim", "lastName": "Althoff", "creatorType": "author" },
      { "firstName": "Noah A.", "lastName": "Smith", "creatorType": "author" },
      {
        "firstName": "Luke",
        "lastName": "Zettlemoyer",
        "creatorType": "author"
      }
    ],
    "tags": [
      { "tag": "Computer Science - Computation and Language", "type": 1 },
      { "tag": "Computer Science - Artificial Intelligence", "type": 1 }
    ],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-26T20:08:57Z",
    "dateModified": "2023-06-26T20:08:57Z"
  },
  {
    "key": "MU89NYCL",
    "version": 97,
    "itemType": "webpage",
    "title": "Moonlight VC Programs",
    "abstractNote": "A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team",
    "url": "https://www.notion.so",
    "accessDate": "2023-06-23T03:54:48Z",
    "websiteTitle": "Notion",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:54:48Z",
    "dateModified": "2023-06-23T03:55:49Z"
  },
  {
    "key": "ZCUVKJUV",
    "version": 31,
    "itemType": "webpage",
    "title": "This is how to train better transformer models",
    "abstractNote": "How to train faster, higher performant transformers",
    "date": "2020-04-15T14:47:15.724Z",
    "language": "en",
    "url": "https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978",
    "accessDate": "2023-06-23T03:27:25Z",
    "websiteTitle": "Medium",
    "creators": [
      { "firstName": "Jonas", "lastName": "Vetterle", "creatorType": "author" }
    ],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:27:25Z",
    "dateModified": "2023-06-23T03:27:25Z"
  },
  {
    "key": "P8PWQDMB",
    "version": 116,
    "itemType": "videoRecording",
    "title": "AlphaGo - The Movie | Full award-winning documentary",
    "abstractNote": "With more board configurations than there are atoms in the universe, the ancient Chinese game of Go has long been considered a grand challenge for artificial intelligence. \n\nOn March 9, 2016, the worlds of Go and artificial intelligence collided in South Korea for an extraordinary best-of-five-game competition, coined The DeepMind Challenge Match. Hundreds of millions of people around the world watched as a legendary Go master took on an unproven AI challenger for the first time in history.\n\nDirected by Greg Kohs and with an original score by Academy Award nominee Hauschka, AlphaGo had its premiere at the Tribeca Film Festival. It has since gone on to win countless awards and near universal praise for a story that chronicles a journey from the halls of Oxford, through the backstreets of Bordeaux, past the coding terminals of DeepMind in London, and ultimately, to the seven-day tournament in Seoul. As the drama unfolds, more questions emerge: What can artificial intelligence reveal about a 3000-year-old game? What can it teach us about humanity?\n\nBest documentary winner: Denver International Film Festival (2017), Warsaw International Film Festival (2017), and Traverse City Film Festival (2017).\n\nOfficial selection at Tribeca Film Festival (2017), BFI London Film Festival (2017), and Critics' Choice Documentary Awards (2017).\n\nFind out more: https://www.alphagomovie.com/\n\n--\n\n\"I want my style of Go to be something different, something new, my own thing, something that no one has thought of before.\" Lee Sedol, Go Champion (18 World Titles).\n\n\"We think of DeepMind as kind of an Apollo program effort for AI. Our mission is to fundamentally understand intelligence and recreate it artificially.\" Demis Hassabis, Co-Founder & CEO, DeepMind.\n\n\"The Game of Go is the holy grail of artificial intelligence. Everything we've ever tried in AI, it just falls over when you try the game of Go.\" Dave Silver, Lead Researcher for AlphaGo.",
    "date": "2020-03-13",
    "libraryCatalog": "YouTube",
    "url": "https://www.youtube.com/watch?v=WXuK6gekU1Y",
    "accessDate": "2023-06-23T04:34:34Z",
    "runningTime": "1:30:27",
    "creators": [{ "name": "Google DeepMind", "creatorType": "director" }],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T04:34:34Z",
    "dateModified": "2023-06-23T04:34:34Z"
  },
  {
    "key": "PGT954R7",
    "version": 67,
    "itemType": "videoRecording",
    "title": "Peter Thiel: You Are Not a Lottery Ticket | Interactive 2013 | SXSW",
    "abstractNote": "Discourse and action in our society are increasingly dominated by the idea that the world cannot be known. But to what degree issuccess in this world dominated by luck? How much of our lives can be planned for, and can the future be achieved in a world dominated by indeterminate thinking? \n\nIn an hour, we'll look at the evolution of determinate to indeterminate thinking in our society, and we'll consider its many implications.",
    "date": "2013-10-14",
    "shortTitle": "Peter Thiel",
    "libraryCatalog": "YouTube",
    "url": "https://www.youtube.com/watch?v=iZM_JmZdqCw",
    "accessDate": "2023-06-23T03:40:44Z",
    "runningTime": "54:01",
    "creators": [{ "name": "SXSW", "creatorType": "director" }],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:40:44Z",
    "dateModified": "2023-06-23T03:40:44Z"
  },
  {
    "key": "XB8PL2VN",
    "version": 2,
    "itemType": "webpage",
    "title": "The Business of Extracting Knowledge from Academic Publications",
    "abstractNote": "After working on biomedical literature search, discovery and recommender web applications for many months I concluded that extracting, structuring or synthesizing \"insights\" from academic publications (papers) has negligible value in industry.",
    "date": "2021-12-07T10:37:00.000Z",
    "language": "en",
    "url": "https://markusstrasser.org/extracting-knowledge-from-literature/",
    "accessDate": "2023-06-19T02:37:43Z",
    "websiteTitle": "Site of Markus Strasser",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-19T02:37:43Z",
    "dateModified": "2023-06-19T02:37:43Z"
  },
  {
    "key": "PYFP6P9W",
    "version": 131,
    "itemType": "webpage",
    "title": "Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases.",
    "abstractNote": "A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team",
    "url": "https://www.notion.so",
    "accessDate": "2023-06-25T07:31:23Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-25T07:31:23Z",
    "dateModified": "2023-06-25T07:31:23Z"
  },
  {
    "key": "U99RBGGT",
    "version": 77,
    "itemType": "webpage",
    "title": "Overview ‚Äî Panel v1.1.1",
    "url": "https://panel.holoviz.org/",
    "accessDate": "2023-06-23T03:46:50Z",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:46:50Z",
    "dateModified": "2023-06-23T03:46:50Z"
  },
  {
    "key": "UZDTD2KG",
    "version": 129,
    "itemType": "attachment",
    "title": "587 VC Firms in SF.xlsx",
    "linkMode": "imported_file",
    "contentType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    "charset": "",
    "filename": "587 VC Firms in SF.xlsx",
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-25T07:21:20Z",
    "dateModified": "2023-06-25T07:21:20Z"
  },
  {
    "key": "ZIK4KP33",
    "version": 99,
    "itemType": "webpage",
    "title": "The Holy Grail of Curing DP/DR",
    "abstractNote": "I won't lie, this is a repost of something \"Copeful\" posted way back in 2007. However I found the information inside completely invaluable and think it should be posted and/or maybe pinned so that people can find the information with ease. Here's hoping it helps many people :] Thank you Copeful...",
    "date": "2021-01-14",
    "language": "en-US",
    "url": "https://www.dpselfhelp.com/threads/the-holy-grail-of-curing-dp-dr.20892/",
    "accessDate": "2023-06-23T03:57:07Z",
    "websiteTitle": "Depersonalization Support Forum",
    "creators": [],
    "tags": [],
    "collections": [],
    "relations": {},
    "dateAdded": "2023-06-23T03:57:07Z",
    "dateModified": "2023-06-23T03:57:07Z"
  }
]
